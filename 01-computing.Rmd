# (PART) Computing resources {-}

# My common practices

First, I will go through some of my general practices that will come up repeatedly later in my notes. 

## Environment variables

Often I create environment variables because this  

- Shortens and simplifies commands  
- Makes it easier to repeat commands for different inputs  
- Hides critical credential information that should not be typed out

If you're not familiar an environment variables [here is a quick overview](https://www.geeksforgeeks.org/environment-variables-in-linux-unix/).

For example if you type

```
echo $MY_VAR
```
and hit Enter you should see only an empty line unless there is a variable with this name defined in your session.  

You can set an environment variable like this

```
export MY_VAR=Random
```

Note that there should be no spaces around the `=`.  

Now when you type  

```
echo $MY_VAR
```
and hit Enter you should see the value you set the variable to.

## Temporary files and global `.gitignore`

Throughout the process of building and interacting with the pipeline there will be information we want to make sure we don't post anywhere online and share publicly (even in private Github repos).  

I tend to deal with this by creating temporary files that are never committed and/or pushed to a repository available online. Often I call these `tmp.ini` or `tmp.txt`. This is particularly important for credential information that can give others access to your AWS account, which will have payment information associated with it (ie. if you post this, your account can be hacked and you can wrack up unexpected charges).  

Assuming you have `git` installed on your system (see [here](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) if you don't) you can set your global `.gitignore` file from your home directory (e.g. for a Mac user this might look like `/Users/zeynepenkavi`)  

For example, a global `.gitignore` file that looks like this:

```
.RData
.Rproj.user
.Rhistory
__pycache__
.ipynb_checkpoints/
*/.ipynb_checkpoints/*
.DS_Store
.pem
tmp*
```
should never track or commit changes for files that start with `tmp` or end with `.pem` etc.  

AWS keys that are used to interact between your machine and the cloud are files with the `.pem` extension. Putting this extension in our global `.gitignore` is one way to avoid posting this information online. **I highly recommend creating a similar `.gitignore` file before you use any other code from my notes.**


# Prerequisites

## Tutorials

If you have no experience at all with AWS I highly recommend starting with this [AWS HPC Workshop](https://www.hpcworkshops.com/).

Make sure to complete:  

- [I. Getting Started in the Cloud](https://www.hpcworkshops.com/02-aws-getting-started.html)  
  - Note: I will not be using Cloud9 for anything else in my notes (instead I build, use, delete my own clusters) but it is useful tool to know about when starting up  
- [II. ParallelCluster CLI](https://www.hpcworkshops.com/04-pcluster-cli.html)  
- [IV. Create and HPC Cluster](https://www.hpcworkshops.com/05-create-cluster.html)  
- [V. Simulation with AWS Batch](https://www.hpcworkshops.com/07-aws-batch.html)  

## Installations

### Docker

If you're not familiar with Docker and how images work please go through at least [this tutorial](https://docs.docker.com/get-started/) because Docker images and containers will be heavily used in the rest of this book.

### AWS-CLI

If you run AWS CLI through the Docker image my commands look like 

```
docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/aws amazon/aws-cli s3 sync
```

instead of 

```
aws s3 sync
```

### AWS-Parallelcluster  

# Interacting with S3

- Transfer data to S3
  - Single file  
  ```
  export STUDY_DIR=/Users/zeynepenkavi/Documents/RangelLab/DescribedVsLearned_fmri/preproc
  docker run --rm -it -v ~/.aws:/root/.aws -v $STUDY_DIR/00_aws:/home amazon/aws-cli s3 cp /home/test-setup-env.sh s3://described-vs-experienced/test-setup-env.sh
  ```

  - One subject folder  
  **Note: mounting the whole raw data folder uses a lot of CPU so you should prob copy the directory you want to copy into a temporary empty dir first**  
  
  ```
  export TMP_DIR=/Users/zeynepenkavi/Downloads/tmp
  mkdir $TMP_DIR
  cp -r /Users/zeynepenkavi/Downloads/GTavares_2017_arbitration/raw_fmri_data/AR-GT-BUNDLES-07_RANGEL $TMP_DIR/AR-GT-BUNDLES-07_RANGEL
  cd $TMP_DIR
  docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/aws amazon/aws-cli s3 sync /aws/AR-GT-BUNDLES-07_RANGEL s3://described-vs-experienced/raw_fmri_data/AR-GT-BUNDLES-07_RANGEL --exclude "*.DS_Store"
  ```

- Check if transfer is successful. Trailing "/" matters for the content  

```
aws s3 ls s3://described-vs-experienced/bids_nifti_wface/
```

- Download data from S3. Note change in command because normally you're running `aws` as an alias but need to make sure that a volume to download is attached when syncing from S3
```
docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/cluster_scripts amazon/aws-cli s3 sync s3://described-vs-experienced/ddModels/cluster_scripts/optim_out /cluster_scripts/optim_out
```

# Setup cluster

This assumes that you have gone through [this tutorial mentioned above](https://www.hpcworkshops.com/03-hpc-aws-parallelcluster-workshop.html) and installed `aws-parallelcluster` on your system.

**NOTE: THESE INSTRUCTIONS ARE FOR ParallelCluster VERSION 2.X. THEY NEED TO BE UPDATED FOR 3.X**

Use custom bootstrap actions to set up master and compute nodes

- Define env variables
```
export KEY_NAME=`aws ec2 describe-key-pairs | jq -j '.KeyPairs[0].KeyName'`
export SG_ID=`aws ec2 describe-security-groups --filters Name=group-name,Values="test-cluster"  | jq -j '.SecurityGroups[0].GroupId'`
export SUBNET_ID=`aws ec2 describe-subnets | jq -j '.Subnets[0].SubnetId'`
export VPC_ID=`aws ec2 describe-vpcs | jq -j '.Vpcs[0].VpcId'`
export REGION=`aws configure get region`
```

- Copy script with [bootstrap actions](https://docs.aws.amazon.com/parallelcluster/latest/ug/pre_post_install.html) to s3
```
export STUDY_DIR=/Users/zeynepenkavi/Documents/RangelLab/DescribedVsLearned_fmri/preproc/00_aws
cd $STUDY_DIR
docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/aws amazon/aws-cli s3 cp /aws/test-setup-env.sh s3://described-vs-experienced/test-setup-env.sh
```

- Set up temporary cluster config file with the environment variables piped in. Note [`make_cluster_config_ini.sh`](https://github.com/zenkavi/DescribedVsLearned_fmri/blob/master/preproc/00_aws/make_cluster_config_ini.sh) creates a `tmp.ini` file with the values piped in. Since I don't want to share these values publicly this file is not committed to my git history through a global setting in my `~/.gitignore` (add `tmp*` to `~/.gitignore`).
```
./make_cluster_config_ini.sh
```

- Create cluster using temporary custom config.
```
pcluster create test-cluster -c tmp.ini
```

- Check cluster status
```
pcluster list --color
```

- Log onto cluster. You can directly ssh to the master node, but the compute nodes are only accessible from the master node, not from the Internet.
```
pcluster ssh test-cluster -i $KEYS_PATH/test-cluster.pem
```

- Stop and start compute nodes of cluster
```
pcluster stop test-cluster

pcluster start test-cluster
```

- Update cluster
```
pcluster update test-cluster -c tmp.ini
```

- Delete cluster
```
pcluster delete test-cluster
```
