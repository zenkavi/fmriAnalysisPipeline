# (PART) Computing resources {-}

# General approach

In this chapter I will go through some of my general practices that will come up repeatedly later in the book. 

**Environment variables**  

Often I will create environment variables
  - Shortens and simplifies commands
  - Makes it easier to repeat commands for different parameters

**Global `.gitignore`**

Throughout the process of building and interacting with the pipeline there will be information we want to make sure we don't post anywhere online and share publicly (even in private Github repos).

- Creating temporary files that are never committed and/or pushed to a repository available online. This is particularly important for credential information that can give others access to your AWS account, which will have payment information associated with it (ie. if you post this, you account can be hacked and you can wrack up unexpected charges).  

Assuming you have `git` installed on your system (see [here](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) if you don't) you can set your global `.gitignore` file from your home directory (e.g. for a Mac user this might look like `/Users/zeynepenkavi`)  

For example, a global `.gitignore` file that looks like this:

```
.RData
.Rproj.user
.Rhistory
__pycache__
.ipynb_checkpoints/
*/.ipynb_checkpoints/*
.DS_Store
.pem
tmp*
```
should never track or commit changes for files that start with `tmp` or end with `.pem` etc.  

AWS keys that use to interact between your machine and the cloud are files with the `.pem` extension. Putting this extension in our global `.gitignore` is one way to avoid posting this information online.


# Prerequisites

## Tutorials

If you're not familiar with AWS please complete the chapters ["Getting Started in the Cloud"](https://www.hpcworkshops.com/02-aws-getting-started.html) and ["Create an HPC Cluster"](https://www.hpcworkshops.com/03-hpc-aws-parallelcluster-workshop.html) in this [AWS HPC Workshop](https://www.hpcworkshops.com/).

After completing these tutorials you should be familiar with:  
- AWS dashboard  
- S3 and how to create buckets  
- Creating and deleting EC2 instances, SSHing into them
- Have [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html) installed on your system  
  

## Installations

### Docker

If you're not familiar with Docker and how images work please go through at least [this tutorial](https://docs.docker.com/get-started/) because Docker images and containers will be heavily used in the rest of this book.

### AWS-CLI

If you run AWS CLI through the Docker image my commands look like 

```
docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/aws amazon/aws-cli s3 sync
```

instead of 

```
aws s3 sync
```

### AWS-Parallelcluster  

# Interacting with S3

- Transfer data to S3
  - Single file  
  ```
  export STUDY_DIR=/Users/zeynepenkavi/Documents/RangelLab/DescribedVsLearned_fmri/preproc
  docker run --rm -it -v ~/.aws:/root/.aws -v $STUDY_DIR/00_aws:/home amazon/aws-cli s3 cp /home/test-setup-env.sh s3://described-vs-experienced/test-setup-env.sh
  ```

  - One subject folder  
  **Note: mounting the whole raw data folder uses a lot of CPU so you should prob copy the directory you want to copy into a temporary empty dir first**  
  
  ```
  export TMP_DIR=/Users/zeynepenkavi/Downloads/tmp
  mkdir $TMP_DIR
  cp -r /Users/zeynepenkavi/Downloads/GTavares_2017_arbitration/raw_fmri_data/AR-GT-BUNDLES-07_RANGEL $TMP_DIR/AR-GT-BUNDLES-07_RANGEL
  cd $TMP_DIR
  docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/aws amazon/aws-cli s3 sync /aws/AR-GT-BUNDLES-07_RANGEL s3://described-vs-experienced/raw_fmri_data/AR-GT-BUNDLES-07_RANGEL --exclude "*.DS_Store"
  ```

- Check if transfer is successful. Trailing "/" matters for the content  

```
aws s3 ls s3://described-vs-experienced/bids_nifti_wface/
```

- Download data from S3. Note change in command because normally you're running `aws` as an alias but need to make sure that a volume to download is attached when syncing from S3
```
docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/cluster_scripts amazon/aws-cli s3 sync s3://described-vs-experienced/ddModels/cluster_scripts/optim_out /cluster_scripts/optim_out
```

# Setup cluster

This assumes that you have gone through [this tutorial mentioned above](https://www.hpcworkshops.com/03-hpc-aws-parallelcluster-workshop.html) and installed `aws-parallelcluster` on your system.

**NOTE: THESE INSTRUCTIONS ARE FOR ParallelCluster VERSION 2.X. THEY NEED TO BE UPDATED FOR 3.X**

Use custom bootstrap actions to set up master and compute nodes

- Define env variables
```
export KEY_NAME=`aws ec2 describe-key-pairs | jq -j '.KeyPairs[0].KeyName'`
export SG_ID=`aws ec2 describe-security-groups --filters Name=group-name,Values="test-cluster"  | jq -j '.SecurityGroups[0].GroupId'`
export SUBNET_ID=`aws ec2 describe-subnets | jq -j '.Subnets[0].SubnetId'`
export VPC_ID=`aws ec2 describe-vpcs | jq -j '.Vpcs[0].VpcId'`
export REGION=`aws configure get region`
```

- Copy script with [bootstrap actions](https://docs.aws.amazon.com/parallelcluster/latest/ug/pre_post_install.html) to s3
```
export STUDY_DIR=/Users/zeynepenkavi/Documents/RangelLab/DescribedVsLearned_fmri/preproc/00_aws
cd $STUDY_DIR
docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/aws amazon/aws-cli s3 cp /aws/test-setup-env.sh s3://described-vs-experienced/test-setup-env.sh
```

- Set up temporary cluster config file with the environment variables piped in. Note [`make_cluster_config_ini.sh`](https://github.com/zenkavi/DescribedVsLearned_fmri/blob/master/preproc/00_aws/make_cluster_config_ini.sh) creates a `tmp.ini` file with the values piped in. Since I don't want to share these values publicly this file is not committed to my git history through a global setting in my `~/.gitignore` (add `tmp*` to `~/.gitignore`).
```
./make_cluster_config_ini.sh
```

- Create cluster using temporary custom config.
```
pcluster create test-cluster -c tmp.ini
```

- Check cluster status
```
pcluster list --color
```

- Log onto cluster. You can directly ssh to the master node, but the compute nodes are only accessible from the master node, not from the Internet.
```
pcluster ssh test-cluster -i $KEYS_PATH/test-cluster.pem
```

- Stop and start compute nodes of cluster
```
pcluster stop test-cluster

pcluster start test-cluster
```

- Update cluster
```
pcluster update test-cluster -c tmp.ini
```

- Delete cluster
```
pcluster delete test-cluster
```
