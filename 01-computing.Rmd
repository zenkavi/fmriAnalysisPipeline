# (PART) Computing resources {-}

# My common practices {#common-practices}

First, I will go through some of my general practices that will come up repeatedly later in my notes. 

## Environment variables

Often I create environment variables because this  

- Shortens and simplifies commands  
- Makes it easier to repeat commands for different inputs  
- Hides critical credential information that should not be typed out

If you're not familiar an environment variables [here is a quick overview](https://www.geeksforgeeks.org/environment-variables-in-linux-unix/).

For example if you type

```
echo $MY_VAR
```
and hit Enter you should see only an empty line unless there is a variable with this name defined in your session.  

You can set an environment variable like this

```
export MY_VAR=Random
```

Note that there should be no spaces around the `=`.  

Now when you type  

```
echo $MY_VAR
```
and hit Enter you should see the value you set the variable to.

## Temporary files and global `.gitignore`

Throughout the process of building and interacting with the pipeline there will be information we want to make sure we don't post anywhere online and share publicly (even in private Github repos).  

I tend to deal with this by creating temporary files that are never committed and/or pushed to a repository available online. Often I call these `tmp.ini` or `tmp.txt`. This is particularly important for credential information that can give others access to your AWS account, which will have payment information associated with it (ie. if you post this, your account can be hacked and you can wrack up unexpected charges).  

Assuming you have `git` installed on your system (see [here](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) if you don't) you can set your global `.gitignore` file from your home directory (e.g. for a Mac user this might look like `/Users/zeynepenkavi`)  

For example, a global `.gitignore` file that looks like this:

```
.RData
.Rproj.user
.Rhistory
__pycache__
.ipynb_checkpoints/
*/.ipynb_checkpoints/*
.DS_Store
.pem
tmp*
```
should never track or commit changes for files that start with `tmp` or end with `.pem` etc.  

AWS keys that are used to interact between your machine and the cloud are files with the `.pem` extension. Putting this extension in our global `.gitignore` is one way to avoid posting this information online. **I highly recommend creating a similar `.gitignore` file before you use any other code from my notes.**


# Prerequisites

## Tutorials

If you have no experience at all with AWS I highly recommend starting with this [AWS HPC Workshop](https://www.hpcworkshops.com/).

Make sure to complete:  

- [I. Getting Started in the Cloud](https://www.hpcworkshops.com/02-aws-getting-started.html)  
  - Note: I will not be using Cloud9 for anything else in my notes (instead I build, use, delete my own clusters) but it is useful tool to know about when starting up  
  - Creating key-pairs, using the AWS CLI, S3 buckets, EC2 instances with correct permissions that you can ssh into are the crucial building blocks for what is to come.  
- [III. ParallelCluster CLI](https://www.hpcworkshops.com/04-pcluster-cli.html)  
  - There is an earlier tutorial that walks through ParallelCluster Manager a GUI to specify the details and deploy your cluster. I haven't used this and don't plan to because I prefer having 
  If you would like to familiarize yourself with how things look like in various AWS dashboards when you deploy a cluster you're welcome to go through it.
- [IV. Create and HPC Cluster](https://www.hpcworkshops.com/05-create-cluster.html)  
- [V. Simulation with AWS Batch](https://www.hpcworkshops.com/07-aws-batch.html)  

Note: I have forked a copy of these tutorials (for when these notes were compiled) to my own Github in case in the future these tutorials are deleted by owners. You can find them [here](https://github.com/zenkavi/aws-hpc-tutorials).  

If you're not familiar with Docker and how images work please go through at least [this tutorial](https://docs.docker.com/get-started/) because Docker images and containers will be heavily used in the rest of these notes.

## Installations

### Docker

### AWS-CLI {#installations-awscli}

If you run AWS CLI through the Docker image my commands look like 

```
docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/aws amazon/aws-cli s3 sync
```

instead of 

```
aws s3 sync
```

### AWS-Parallelcluster  

### JQ {#installations-jq}

# Interacting with S3

## Transfer data 
  - Single file  
  ```
  export STUDY_DIR=/Users/zeynepenkavi/Documents/RangelLab/DescribedVsLearned_fmri/preproc
  docker run --rm -it -v ~/.aws:/root/.aws -v $STUDY_DIR/00_aws:/home amazon/aws-cli s3 cp /home/test-setup-env.sh s3://described-vs-experienced/test-setup-env.sh
  ```

  - One subject folder  
  **Note: mounting the whole raw data folder uses a lot of CPU so you should prob copy the directory you want to copy into a temporary empty dir first**  
  
  ```
  export TMP_DIR=/Users/zeynepenkavi/Downloads/tmp
  mkdir $TMP_DIR
  cp -r /Users/zeynepenkavi/Downloads/GTavares_2017_arbitration/raw_fmri_data/AR-GT-BUNDLES-07_RANGEL $TMP_DIR/AR-GT-BUNDLES-07_RANGEL
  cd $TMP_DIR
  docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/aws amazon/aws-cli s3 sync /aws/AR-GT-BUNDLES-07_RANGEL s3://described-vs-experienced/raw_fmri_data/AR-GT-BUNDLES-07_RANGEL --exclude "*.DS_Store"
  ```

## Check transfer  

Trailing "/" matters for the content  

```
aws s3 ls s3://described-vs-experienced/bids_nifti_wface/
```

## Download data  

Note change in command because normally you're running `aws` as an alias but need to make sure that a volume to download is attached when syncing from S3
```
docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/cluster_scripts amazon/aws-cli s3 sync s3://described-vs-experienced/ddModels/cluster_scripts/optim_out /cluster_scripts/optim_out
```

# Setup cluster

## Prerequisites  

- [ParallelCluster CLI tutorial](https://www.hpcworkshops.com/04-pcluster-cli/)  
- [HPC Cluster tutorial](https://www.hpcworkshops.com/05-create-cluster.html)  
- `aws-parallelcluster` (version >3) [installed on your system](#installations-awscli)  
- `jq` [installed on your system](#installations-jq)

## Bootstrap actions

[Bootstrap actions](https://docs.aws.amazon.com/parallelcluster/latest/ug/custom-bootstrap-actions-v3.html) allow you to setup your head and compute nodes however you'd like. With bootstrap actions you can install scripts, setup aliases, download data from S3 etc. You can have different setup scripts for head and compute nodes. Typically these are placed in S3 and when configuring your cluster given access to for the relevant nodes in your config file (as described later).

For example, you can have a script to pull a Docker image and create some directories in your head node with a script [`head-node-setup-env.sh`](https://github.com/zenkavi/fmriAnalysisPipeline/blob/main/resources/head-node-setup-env.sh)

```
#! /bin/bash
amazon-linux-extras install docker -y
service docker start
usermod -a -G docker ec2-user
chkconfig docker on
docker pull zenkavi/fsl:6.0.3

mkdir /shared/.out
mkdir /shared/.err

export TEST_PATH=/shared/

aws s3 cp s3://zenkavi/test_setup.txt $TEST_PATH

cat $TEST_PATH/test_setup.txt

chown -R ec2-user: /shared

echo "alias squeue='squeue -o \"%.18i %.9P %.18j %.8u %.2t %.10M %.6D %R\"'">> /home/ec2-user/.bash_profile
```

And copy this script to s3
```
docker run --rm -it -v ~/.aws:/root/.aws -v $(pwd):/aws amazon/aws-cli s3 cp /aws/head-node-setup-env.sh s3://zenkavi/head-node-setup-env.sh
```

to use in your cluster config file.  

Some version of bootstrap action scripts will almost certainly be necessary for any clusters you make, since otherwise these are very bare-bone machines without anything else installed in them.  

## Cluster config file

[My example file](https://github.com/zenkavi/fmriAnalysisPipeline/blob/main/resources/make_tmp_cluster_config.sh)

- Set up temporary cluster config file with the environment variables piped in. Note [`make_cluster_config_ini.sh`](https://github.com/zenkavi/DescribedVsLearned_fmri/blob/master/preproc/00_aws/make_cluster_config_ini.sh) creates a `tmp.ini` file with the values piped in. [Since I don't want to share these values publicly](#common-practices) this file is not committed to my git history through a global setting in my `~/.gitignore` (add `tmp*` to `~/.gitignore`).
```
./make_cluster_config_ini.sh
```

## Create and use cluster

### Create cluster using temporary custom config
```
pcluster create-cluster test-cluster -c tmp.yaml
```

### Check cluster status
```
pcluster list --color
```

### Log onto cluster

You can directly ssh to the head node, but the compute nodes are only accessible from the head node, not from the Internet.
```
pcluster ssh test-cluster -i $KEYS_PATH/test-cluster.pem
```

Note that `$KEYS_PATH` is an environment variable that points to where your aws keys are stored on your system.

### Stop and start compute nodes of cluster
```
pcluster stop test-cluster

pcluster start test-cluster
```

### Update cluster

If you want to change some configurations of your cluster ...

This is not possible for all options.

```
pcluster update test-cluster -c tmp.ini
```

### Delete cluster
```
pcluster delete test-cluster
```
